{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Classification\n",
    "* Data pipline\n",
    "* Model structure and design\n",
    "* Model Compilation\n",
    "* Training/Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.1' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# import the needed modules\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pipeline\n",
    "* Read and load the data \n",
    "* understand the data\n",
    "* remove the noise and normalize the data\n",
    "* Define batches\n",
    "* Convert to tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST digits classification dataset \n",
    "This is a dataset of 60,000 28X28 grayscale images of the 10 digits, along with a test set of 10,000 images More info can be found at the MNIST homepage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "assert x_train.shape == (60000, 28, 28)\n",
    "assert x_test.shape == (10000, 28, 28)\n",
    "assert y_train.shape == (60000,)\n",
    "assert y_test.shape == (10000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Have a look at the data ! ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(2,5, figsize=(15, 8))\n",
    "ax = ax.flatten()\n",
    "for i in range(10):\n",
    "    im_idx = np.argwhere(y_train == i)[0]\n",
    "    plottable_image = np.reshape(x_train[im_idx], (28, 28))\n",
    "    ax[i].imshow(plottable_image, cmap='gray_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "Convert labels to one-Hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we have a class number for each image\n",
    "print(\"Class label of first image :\", y_train[0])\n",
    "\n",
    "# Convert to One-Hot Encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10) \n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "print(\"After converting the output into a vector : \",y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the image pixel value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[0].min(), x_train[0].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the data between [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train/255.0\n",
    "x_test = x_test/255.0\n",
    "\n",
    "x_train[0].min(), x_train[0].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the shapes for the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape the images so each image is one vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(-1, 28*28)\n",
    "x_test = x_test.reshape(-1, 28*28)\n",
    "\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "Building the neural network requires configuring the layers of the model, then compiling the model. First we stack a few layers together using keras.Sequential. Next we configure the loss function, optimizer, and metrics to monitor. These are added during the model's compile step:\n",
    "* Loss function - measures how accurate the model is during training, we want to minimize this with the optimizer.\n",
    "* Optimizer - how the model is updated based on the data it sees and its loss function.\n",
    "* Metrics - used to monitor the training and testing steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the optimizer\n",
    "adam_opt = tf.keras.optimizers.Adam(learning_rate=0.01,\n",
    "                                    beta_1=0.9,\n",
    "                                    beta_2=0.999)\n",
    "#define the loss\n",
    "loss_ = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "#define the number of units per layer\n",
    "feature_size = 784\n",
    "unit_size = 500\n",
    "n_hidden_layers = 0\n",
    "\n",
    "def build_mlp_model(feature_size, n_hidden_layers, unit_size, opt_, loss_):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(unit_size, activation=tf.nn.relu, input_shape=(feature_size,))        \n",
    "    ])\n",
    "\n",
    "    for i in range(n_hidden_layers):\n",
    "      model.add(tf.keras.layers.Dense(unit_size, \n",
    "                                      activation=tf.nn.relu,\n",
    "                                      kernel_initializer=tf.keras.initializers.HeUniform(),\n",
    "                                      bias_initializer=tf.keras.initializers.Zeros(),\n",
    "                                      kernel_regularizer=tf.keras.regularizers.L1L2(l1=1e-5, l2=1e-4),\n",
    "                                      bias_regularizer=tf.keras.regularizers.L2(1e-4),\n",
    "                                      ))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(10))\n",
    "    \n",
    "    model.compile(optimizer=opt_, \n",
    "                  loss=loss_,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "mlp = build_mlp_model(feature_size, n_hidden_layers, unit_size, adam_opt, loss_)\n",
    "\n",
    "mlp.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Train the model** \n",
    "Training the neural network model requires the following steps:\n",
    "1. Feed the training data to the modelâ€”in this example, the train features and train labels arrays.\n",
    "2. The model learns to associate features and labels.\n",
    "3. We ask the model to make predictions about a test setâ€”in this example, the test_features array. We verify that the predictions match the labels from the test_labels array.\n",
    "\n",
    "To start training, call the model.fit method—the model is \"fit\" to the training data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50)\n",
    "history = mlp.fit(x_train, y_train, \n",
    "                    epochs=50, \n",
    "                    verbose=1,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data = (x_test, y_test),\n",
    "                    callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the training and validation error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "\n",
    "hist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(hist_):\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.xlabel('Epoch',fontsize=20)\n",
    "    plt.ylabel('Accuracy',fontsize=20)\n",
    "    plt.plot(hist['epoch'], hist['accuracy'], label='Train Error')\n",
    "    plt.plot(hist['epoch'], hist['val_accuracy'], label = 'Val Error')\n",
    "    plt.legend([\"train\", \"validation\"], loc=\"upper left\", prop={'size': 20})\n",
    "\n",
    "\n",
    "plot_history(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would you change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "End of notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
