{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP  Regression\n",
    "* Data pipline\n",
    "* Model structure and design\n",
    "* Model Compilation \n",
    "* Training / Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# import the needed modules\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pipeline\n",
    "* Read and load that data\n",
    "* Understand the data \n",
    "* Remove noise and normalize the data \n",
    "* Define batches \n",
    "* convert to tensors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting House Prices on Kaggle\n",
    "Now that we have introduced some basic tools for building and training deep networks and regularizing them with techniques including weight decay and dropout, we are ready to put all this knowledge into practice by participating in a Kaggle competition. The house price prediction competition is a great place to start. The data is fairly generic and do not exhibit exotic structure that might require specialized models (as audio or video might). This dataset covers house prices in Ames, IA from the period of 2006--2010.\n",
    "It is considerably larger than the famous [Boston housing dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names) of Harrison and Rubinfeld (1978), boasting both more examples and more features. In this section, we will walk you through details of data preprocessing, model design, and hyperparameter selection. We hope that through a hands-on approach, you will gain some intuitions that will guide you in your career as a data scientist.\n",
    "\n",
    "* The training dataset includes 1460 examples, 80 features, and 1 label\n",
    "* The validation data contains 1459 examples and 80 features.\n",
    "\n",
    "On the house price prediction competition page, you can find the dataset (under the \\\"Data\\\" tab), submit predictions, and see your ranking, The URL is right here: > https://www.kaggle.com/c/house-prices-advanced-regression-techniques ![The house price prediction competition page.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Make a data directory to store the data!\n",
    "os.makedirs(os.path.join('.', 'data'), exist_ok=True)\n",
    "\n",
    "\n",
    "#Thanks to D2L! You can also download from Kaggle\n",
    "dataname = \"HousePrices\"\n",
    "raw_train_url = \"http://d2l-data.s3-accelerate.amazonaws.com/kaggle_house_pred_train.csv\"\n",
    "raw_test_url = \"http://d2l-data.s3-accelerate.amazonaws.com/kaggle_house_pred_test.csv\"\n",
    "\n",
    "\n",
    "\n",
    "def load_data(train_url, test_url, name, folder=\".\", save_data=False):\n",
    "  raw_train = pd.read_csv(train_url)\n",
    "  raw_test = pd.read_csv(test_url)\n",
    "\n",
    "  if save_data:\n",
    "    raw_train.to_csv(folder+name+\"Train.csv\")\n",
    "    raw_test.to_csv(folder+name+\"Test.csv\")\n",
    "\n",
    "  return raw_train, raw_test\n",
    "\n",
    "\n",
    "\n",
    "raw_train, raw_test = load_data(raw_train_url, raw_test_url, dataname, \"data/\", save_data=False)\n",
    "\n",
    "raw_train.shape, raw_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Have a look at the data!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "raw_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here is the columns!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#All columns\n",
    "raw_test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numeric Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#numeric columns\n",
    "numeric_columns = raw_test.dtypes[raw_test.dtypes!='object'].index\n",
    "numeric_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Object columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#object columns\n",
    "object_columns = raw_test.dtypes[raw_test.dtypes=='object'].index\n",
    "object_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understand the data**\n",
    "\n",
    "Plot histogram of numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "'''Plot histogram of numerical variables to validate pandas intuition.'''\n",
    "\n",
    "def draw_histograms(df, variables, n_rows, n_cols):\n",
    "    fig=plt.figure()\n",
    "    \n",
    "    for i, var_name in enumerate(variables):\n",
    "        ax=fig.add_subplot(n_rows,n_cols,i+1)\n",
    "        df[var_name].hist(bins=40,ax=ax,color = 'blue',alpha=0.7, figsize = (40, 60))\n",
    "        ax.set_title(var_name, fontsize = 30)\n",
    "        ax.tick_params(axis = 'both', which = 'major', labelsize = 20)\n",
    "        ax.tick_params(axis = 'both', which = 'minor', labelsize = 20)\n",
    "        ax.set_xlabel('')\n",
    "    fig.tight_layout(rect = [0, 0.03, 1, 0.95])  # Improves appearance a bit.\n",
    "    plt.show()\n",
    "    \n",
    "draw_histograms(raw_train[numeric_columns].drop(columns=['Id']), numeric_columns[1:], 9, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explore some columns and their correlations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "explore_columns = [\"LotArea\", \"TotalBsmtSF\", \"GarageArea\", \"ScreenPorch\", \"PoolArea\",\"SalePrice\"]\n",
    "\n",
    "\n",
    "corr = raw_train[explore_columns].corr()\n",
    "f, ax = plt.subplots(figsize=(15, 12))\n",
    "sns.heatmap(corr, linewidths=.5, vmax=1, square=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explore some columns and their correlation to SalePrice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "k = 10 #number of variables for heatmap\n",
    "cols = corr.nlargest(k, 'SalePrice')['SalePrice'].index\n",
    "cm = np.corrcoef(raw_train[cols].values.T)\n",
    "f, ax = plt.subplots(figsize=(15, 12))\n",
    "sns.set(font_scale=1.5)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 20}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General correlation heatmap for columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "corr = raw_train[numeric_columns].corr()\n",
    "f, ax = plt.subplots(figsize=(15, 12))\n",
    "sns.set(font_scale=1)\n",
    "sns.heatmap(corr, linewidths=.5, vmax=1, square=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Preprocessing**\n",
    "We can see that in each example, the first feature is the ID. This helps the model identify each training example. While this is convenient, it does not carry any information for prediction purposes. Hence, we will remove it from the dataset before feeding the data into the model. Besides, given a wide variety of data types, we will need to preprocess the data before we can start modeling. Let's start with the numerical features. First, we apply a heuristic, [**replacing all missing values by the corresponding feature's mean.**] Then, to put all features on a common scale, we (**standardize the data by rescaling features to zero mean and unit variance**): $$ x \\\\leftarrow \\\\frac{x - \\\\mu}{\\\\sigma}, $$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
