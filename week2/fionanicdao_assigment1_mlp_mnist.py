# -*- coding: utf-8 -*-
"""fionaNicdao_assigment1_mlp_mnist.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1--KksRIg_z-pSo0QzDXijRq66ya0mU0V

# Fiona Nicdao's Assignment 1
"""

import numpy as np
import tensorflow as tf
import pandas as pd
from tensorflow import  keras
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import time

"""## Processing the MNIST Dataset"""

#build the model based on the data

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

# Change the data to be split into 70% training set and 30% testing set
x = np.concatenate((x_train, x_test))
y = np.concatenate((y_train, y_test))
train_size = 0.7
x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=train_size,
                                                    random_state=42)

dev_size = 0.8 * x_train.shape[0]
dev_size = int(dev_size)

#shuffle the x_train (good practice)
#seed for reproducibility
indices = np.arange(x_train.shape[0])
np.random.shuffle(indices)
x_train = x_train[indices]
y_train = y_train[indices]

# plot the image
plt.imshow(x_train[0], cmap='gray')
plt.show()

#dividing the training dataset into 80/20 : training set/ validation set
x_val = x_train[dev_size:] #validation sets
y_val = y_train[dev_size:]

x_train = x_train[:dev_size] #training sets
y_train = y_train[:dev_size]

#preparing training data
#dividing them by max pixel value as a float to get all values btw 0 and 1
x_train = (x_train/255.0).reshape(-1, 28*28)
x_val = (x_val/255.0).reshape(-1, 28*28)
x_test = (x_test/255.0).reshape(-1, 28*28)

#make the classes one-hot encodings
y_train = tf.keras.utils.to_categorical(y_train)
y_val = tf.keras.utils.to_categorical(y_val)
y_test = tf.keras.utils.to_categorical(y_test)

print(x_train.shape) #6000 training samples, image is 28x28 size
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)

#check step that the data is normalized between [1.0, 0.0]
x_train[0].max(), x_train[0].min()
# better to have it float values /

"""## Building the Model : MLP"""

#model
class MLP(tf.keras.Model):
  def __init__(self, num_classes, input_shape, n_layers, n_units, activation,
               optim, loss, initializer,reg):
      super(MLP, self).__init__()
      self.num_classes = num_classes
      self.input_shape = input_shape
      self.n_layers = n_layers
      self.n_units = n_units
      self.activation = activation
      self.optimizer = optim
      self.loss = loss
      self.initializer = initializer
      self.regularizer = reg

      self.model = self.create_model()

  #build the structure of the model
  def create_model(self):
    model = tf.keras.Sequential() # Sequential model is just a placeholder
    model.add(tf.keras.layers.Input(shape=self.input_shape))

    for i in range(self.n_layers):
      model.add(tf.keras.layers.Dense(self.n_units,
                                      input_shape=self.input_shape,
                                      activation=self.activation,
                                      kernel_initializer = self.initializer,
                                      kernel_regularizer= self.regularizer))

    model.add(tf.keras.layers.Dense(self.num_classes, activation='softmax'))

    return model

  def compile_model(self):
    self.model.compile(optimizer=self.optimizer, loss=self.loss,
                       metrics=['accuracy'])

  def train_model(self, x_train, y_train, x_val, y_val, epochs=10,
                  batch_size=32):
    self.model.fit(x_train, y_train, epochs=epochs, batch_size=64,
                   validation_data=(x_val, y_val))

  def evaluate_model(self, x_test, y_test):
    test_loss, test_acc = self.model.evaluate(x_test, y_test)
    return test_loss, test_acc

"""## TASK 1"""

mlp_task1 = MLP(num_classes=10,
          input_shape=(28*28,),
          n_layers=2,
          n_units=100,
          activation='relu',
          optim= tf.keras.optimizers.SGD(learning_rate=0.0001),
          loss=tf.keras.losses.CategoricalCrossentropy(),
          initializer=tf.keras.initializers.HeNormal(),
          reg = tf.keras.regularizers.l2(0.001))
mlp_task1.compile_model()
start = time.time()
mlp_task1.train_model(x_train, y_train, x_val, y_val, epochs=50, batch_size=32)
end = time.time()
print(f"Training time for MLP task 1: {end - start} seconds")

mlp_task1.evaluate_model(x_test, y_test)

mlp_task1.summary()

"""## TASK 2"""

# make a dataframe to compare the accuracy and loss for all the different activation
df = pd.DataFrame(columns=['Activation', 'Loss', 'Accuracy','Time'])

"""### Sigmoid activation"""

mlp_sigmoid = MLP(num_classes=10,
          input_shape=(28*28,),
          n_layers=2,
          n_units=100,
          activation='sigmoid',
          optim= tf.keras.optimizers.SGD(learning_rate=0.0001),
          loss=tf.keras.losses.CategoricalCrossentropy(),
          initializer=tf.keras.initializers.RandomNormal(),
          reg = tf.keras.regularizers.l2(0.001))
mlp_sigmoid.compile_model()
start = time.time()
mlp_sigmoid.train_model(x_train, y_train, x_val, y_val, epochs=50, batch_size=32)
end = time.time()
print(f"Training time for MLP task 2 sigmoid activation: {end - start} seconds")
print("\n")
test_loss, test_acc = mlp_sigmoid.evaluate_model(x_test, y_test)
df.loc[len(df)] =['sigmoid',test_loss, test_acc, end - start]
mlp_sigmoid.summary()

"""### Tanh activation"""

mlp_tanh = MLP(num_classes=10,
          input_shape=(28*28,),
          n_layers=2,
          n_units=100,
          activation='tanh',
          optim= tf.keras.optimizers.SGD(learning_rate=0.0001),
          loss=tf.keras.losses.CategoricalCrossentropy(),
          initializer=tf.keras.initializers.RandomNormal(),
          reg = tf.keras.regularizers.l2(0.001))
mlp_tanh.compile_model()
start = time.time()
mlp_tanh.train_model(x_train, y_train, x_val, y_val, epochs=50, batch_size=32)
end = time.time()
print(f"Training time for MLP task 2 tanh activation: {end - start} seconds")
print("\n")
test_loss, test_acc = mlp_tanh.evaluate_model(x_test, y_test)
df.loc[len(df)] =['tanh',test_loss, test_acc, end - start]
mlp_tanh.summary()

"""### Relu activation"""

mlp_relu = MLP(num_classes=10,
          input_shape=(28*28,),
          n_layers=2,
          n_units=100,
          activation='relu',
          optim= tf.keras.optimizers.SGD(learning_rate=0.0001),
          loss=tf.keras.losses.CategoricalCrossentropy(),
          initializer=tf.keras.initializers.RandomNormal(),
          reg = tf.keras.regularizers.l2(0.001))
mlp_relu.compile_model()
start = time.time()
mlp_relu.train_model(x_train, y_train, x_val, y_val, epochs=50, batch_size=32)
end = time.time()
print(f"Training time for MLP task 2 relu activation: {end - start} seconds")
print("\n")
test_loss, test_acc = mlp_relu.evaluate_model(x_test, y_test)
df.loc[len(df)] =['relu',test_loss, test_acc,end - start]
mlp_relu.summary()

"""## Compare the accuracy, loss, time for all the different activation"""

print(df)

"""# Results: Activation = tanh, has the least amount of loss and highest accuracy. But the fastest time varies, sometimes its tanh and other times it is relu. My last fastest time is relu."""